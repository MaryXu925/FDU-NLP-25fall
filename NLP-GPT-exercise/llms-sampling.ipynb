{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed593483-793b-4a9b-add0-df360616b532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xumx/miniconda3/envs/Deepconf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a47064-7fcb-45a6-807d-879150bba42f",
   "metadata": {},
   "source": [
    "### 1. Auto-regressive generation\n",
    "\n",
    "Please see more details in [https://huggingface.co/blog/how-to-generate](https://huggingface.co/blog/how-to-generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05293de1-b04f-4752-a8c6-aebe791661e1",
   "metadata": {},
   "source": [
    "In short, auto-regressive language generation is based on the assumption that the probability distribution of a word sequence can be decomposed into the product of conditional next word distributions:\n",
    "$$\n",
    "p\\left(w_{1: T} \\mid P_0\\right) = \\prod_{t=1}^T p\\left(w_t \\mid w_{1: t-1}, P_0\\right), \\text{ with } w_{1: 0}=\\emptyset,\n",
    "$$\n",
    "where $P_0$ being the initial context word sequence. The length $T$ of the word sequence is usually determined on-the-fly and corresponds to the timestep $t=T$ the EOS token is generated from $p\\left(w_t \\mid w_{1: t-1}, P_0\\right)$. We will give a tour of the currently most prominent decoding methods, mainly Greedy search, Beam search, and Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69834e7b-c578-4217-a8f0-fd358ad4856d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_ENDPOINT set to: https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Configure Hugging Face mirror endpoint (exporting via ! shell does not persist for Python)\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "print(\"HF_ENDPOINT set to:\", os.environ.get(\"HF_ENDPOINT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf97e6-3b4a-4e27-964c-fb028400aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the device.\n",
    "if torch.backends.mps.is_available():\n",
    "    torch_device = \"mps\"          # Apple Silicon GPU (Metal)\n",
    "elif torch.cuda.is_available():\n",
    "    torch_device = \"cuda\"         # NVIDIA GPU\n",
    "else:\n",
    "    torch_device = \"cpu\"          # fallback\n",
    "\n",
    "print(torch_device)\n",
    "\n",
    "# Try to load tokenizer from remote (mirror) first, fallback to manual local construction if offline.\n",
    "from transformers import AutoTokenizer, GPT2TokenizerFast\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "except OSError as e:\n",
    "    print(\"Remote AutoTokenizer load failed (\", e.__class__.__name__, \") -> attempting manual local construction...\")\n",
    "    local_base = \"code-gpt2/code-gpt2\"  # contains encoder.json (vocab) and vocab.bpe (merges)\n",
    "    vocab_file = f\"{local_base}/encoder.json\"\n",
    "    merges_file = f\"{local_base}/vocab.bpe\"\n",
    "    try:\n",
    "        tokenizer = GPT2TokenizerFast(vocab_file=vocab_file, merges_file=merges_file)\n",
    "    except Exception as inner:\n",
    "        raise RuntimeError(f\"Local tokenizer construction failed: {inner}. Ensure files encoder.json and vocab.bpe exist.\")\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49316a-5609-42ba-93cf-9e2c7eb39ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(torch_device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45792b0d-6b8d-41a2-bf3b-12e60c844d2d",
   "metadata": {},
   "source": [
    "#### 1.1 Greedy Search\n",
    "\n",
    "Greedy search is the simplest decoding method. It selects the word with the highest probability as its next word: \n",
    "$$\n",
    "w_t=\\operatorname{argmax}_w p_\\theta\\left(w \\mid w_{1: t-1}\\right),\n",
    "$$ \n",
    "at each timestep $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdae9a4-8cc4-4f32-b5e9-ba70fdfd7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0081d05-c217-4294-9bc3-35336b2b685f",
   "metadata": {},
   "source": [
    "The *model_inputs* contains \n",
    "- input_ids: the token ids after tokenization\n",
    "- attention_mask: a binary tensor with the same length as input_ids, telling the model which tokens are real words and which are padding. During batch training or evaluation (DataLoader or collate_fn). In the case of single-sentence inference, or generation of LLMs, there is no need to padding (meaning all attention masks are 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd4eeec-5397-4f5a-b1e8-1888bb6819b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)\n",
    "# generate 200 new tokens as the result of the inference\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=200)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f09e8b-df66-4303-a489-431705e40eb0",
   "metadata": {},
   "source": [
    "#### 1.2 Beam search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c136141-f9be-47fc-aba5-9a5feb7de8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(**model_inputs, max_new_tokens=200, num_beams=5, early_stopping=True)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b268511-1499-4c25-8106-ddd1f83c6895",
   "metadata": {},
   "source": [
    "#### 1.3 Beam search with N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d521a8-029a-4031-a930-8de00f81c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7bd16-abab-42a0-94fc-90d3188a7eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9cbfd9-bf23-4186-9399-6d96516b04f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    top_k=0,\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0f013-a0dd-49e5-98ab-cd3b7e2b2f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb3124-27ee-4b1b-96b7-321ac77284f0",
   "metadata": {},
   "source": [
    "#### 1.4 With Tempature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf954c-34eb-485f-af3a-dad04770063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42) #设定了随机数种子，别人就可以复现；否则每次的随机数是不一样的\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    top_p=0.92, #按概率从高到低排序，取累计概率刚超过阈值 p 的最小子集合（动态集合大小）。只在该集合内按原始（或温度调整后）分布随机抽样\n",
    "    top_k=50  #每一步采样只在概率最高的 k 个候选 token 中进行\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572df90-2931-4d37-acdb-eab430f1164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f209d1c-a430-4785-a4f6-39c944faaa91",
   "metadata": {},
   "source": [
    "### 2. GPT-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2763bbd-d26d-4589-8ef7-7c51387068ec",
   "metadata": {},
   "source": [
    "#### 2.1 Sampling from GPT-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ef8b4-b91d-4592-9e80-46ebba985d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8871bbd4-bfaa-4e8f-a9ca-3387ab393e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='openai-gpt')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea8083-f21d-4ef9-a726-b65e3519ae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTModel\n",
    "import torch\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "model = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(last_hidden_states[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ef273-6abe-40ac-991d-a505adadf25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,)\n",
    "import evaluate\n",
    "\n",
    "\n",
    "MODEL_NAME = \"openai-gpt\"\n",
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2e4ad-98f5-457b-8dd7-26e39adaaf4c",
   "metadata": {},
   "source": [
    "#### 2.2 Fine-tuning\n",
    "\n",
    "\n",
    "[https://github.com/huggingface/pytorch-openai-transformer-lm](https://github.com/huggingface/pytorch-openai-transformer-lm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deepconf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
