{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf804f9-8d65-47ee-8668-47c0b89ab596",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "\n",
    "> 1. Rename assignment-03-###-###.ipynb where ### is your student ID and your name (Chinese).\n",
    "> 2. The deadline of Assignment-03 is 23:59pm, 11-19-2025\n",
    "> 3. Submit a single ZIP archive that includes every file you downloaded. You only need to modify transformer.py, run the Jupyter notebook, and save the resulting performance output inside the archive.\n",
    "> 4. The primary goal of this assignment is to give you hands-on experience implementing a Transformer model.\n",
    "\n",
    "## Task\n",
    "> In this assignment, you will train a Transformer model to count letters. Given a string of characters, your task is to predict, for each position in the string, how many times the character at that position occurred previously, maxing out at 2. This is a 3-class classification task (with labels 0, 1, or > 2, which we’ll just denote as 2). This task is easy with a rule-based system, but it is not so easy for a model to learn. However, Transformers are ideally set up to be able to “look back” with self-attention to count occurrences in the context. Below is an example string (x) (which ends in a trailing space) and its corresponding labels (y):\n",
    "> - x: i like movies a lot\n",
    "> - y: 00010010002102021102\n",
    ">  \n",
    "> If your implementation is correct, then ```python letter_counting.py --task BEFORE```\n",
    "> gives a reasonable output (accuracy will be above 90%).\n",
    "\n",
    "\n",
    "> We also present a modified version of this task that counts both occurrences of letters before and after in the sequence:\n",
    "> - x: i like movies a lot\n",
    "> - y: 22120120102102021102\n",
    ">  \n",
    "> If your implementation is correct, then ```python letter_counting.py --task BEFOREAFTER ``` gives a reasonable output (accuracy will be above 90%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b7bd2b-8b6f-443f-84b8-3b82f55480b9",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "> The dataset for this homework is derived from the text8 collection, which comes from Wikipedia. Your method will use character-level tokenization and operate over text8 sequences that are each exactly 20 characters long. Only 27 character types are present (lowercase characters and spaces); special characters are replaced by a single space and numbers are spelled out as individual digits (50 becomes five zero). Part of examples are:\n",
    "> \n",
    "> - heir average albedo\n",
    "> - ed by rank and file\n",
    "> - s can also extend in\n",
    "> - erages between nine\n",
    "> - that civilization n\n",
    "> - on a t shaped islan\n",
    "> \n",
    "> The dataset is in lettercounting-train.txt and lettercounting-dev.txt. Both two files contain character strings of length 20. You can assume that your model will always see 20 characters as input and make a prediction at each position in the sequence.\n",
    "\n",
    "## Code\n",
    "\n",
    "> The framework code you are given consists of several files.\n",
    "> 1. *utils.py*: it implements an Indexer class, which can be used to maintain a bijective mapping between indices and features (strings).\n",
    "> 2. *letter_counting.py*: contains the driver code, which imports transformer.py, the file you will be editing for this assignment.\n",
    "> 3. *transformer.py*: **You need to fill out all missing parts. Note that your solutions should not use nn.TransformerEncoder, nn.TransformerDecoder, or any other off-the-shelf self-attention layers. You can use nn.Linear, nn.Embedding, and PyTorch’s provided nonlinearities / loss functions to implement Transformers from scratch.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7a9711-968f-40b0-92a3-80267839c3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(task='BEFORE', train='data/lettercounting-train.txt', dev='data/lettercounting-dev.txt', output_bundle_path='classifier-output.json')\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
      "10000 lines read in\n",
      "1000 lines read in\n",
      "Epoch 1/10 - Loss: 0.345033\n",
      "Epoch 2/10 - Loss: 0.136359\n",
      "Epoch 3/10 - Loss: 0.079902\n",
      "Epoch 4/10 - Loss: 0.060359\n",
      "Epoch 5/10 - Loss: 0.058332\n",
      "Epoch 6/10 - Loss: 0.046585\n",
      "Epoch 7/10 - Loss: 0.057002\n",
      "Epoch 8/10 - Loss: 0.051632\n",
      "Epoch 9/10 - Loss: 0.063634\n",
      "Epoch 10/10 - Loss: 0.066841\n",
      "INPUT 0: heir average albedo \n",
      "GOLD 0: array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 1, 2, 0, 0, 2, 0, 0, 2])\n",
      "PRED 0: array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 1, 2, 0, 0, 2, 0, 0, 2])\n",
      "INPUT 1: ed by rank and file \n",
      "GOLD 1: array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 1, 2, 0, 0, 0, 1, 2])\n",
      "PRED 1: array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 1, 2, 0, 0, 0, 1, 2])\n",
      "INPUT 2: s can also extend in\n",
      "GOLD 2: array([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 1, 1, 0, 2, 0, 2])\n",
      "PRED 2: array([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 1, 1, 0, 2, 0, 2])\n",
      "INPUT 3: erages between nine \n",
      "GOLD 3: array([0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 2, 0, 1, 1, 0, 2, 2, 2])\n",
      "PRED 3: array([0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 2, 0, 1, 1, 0, 1, 2, 2])\n",
      "INPUT 4:  that civilization n\n",
      "GOLD 4: array([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 2, 0, 1, 2, 2, 0, 0, 2, 1])\n",
      "PRED 4: array([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 2, 0, 1, 2, 2, 0, 0, 2, 2])\n",
      "Accuracy: 98 / 100 = 0.980000\n",
      "Training accuracy (100 exs):\n",
      "Accuracy: 1972 / 2000 = 0.986000\n",
      "Dev accuracy (whole set):\n",
      "Decoding on a large number of examples (1000); not printing or plotting\n",
      "Accuracy: 19748 / 20000 = 0.987400\n"
     ]
    }
   ],
   "source": [
    "!python letter_counting.py --task BEFORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161c17de-eeed-4c8f-91e9-6b8e8ce344f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(task='BEFOREAFTER', train='data/lettercounting-train.txt', dev='data/lettercounting-dev.txt', output_bundle_path='classifier-output.json')\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
      "10000 lines read in\n",
      "1000 lines read in\n",
      "Epoch 1/10 - Loss: 0.460718\n",
      "Epoch 2/10 - Loss: 0.212338\n",
      "Epoch 3/10 - Loss: 0.119900\n",
      "Epoch 4/10 - Loss: 0.085283\n",
      "Epoch 5/10 - Loss: 0.064985\n",
      "Epoch 6/10 - Loss: 0.057256\n",
      "Epoch 7/10 - Loss: 0.052590\n",
      "Epoch 8/10 - Loss: 0.045367\n",
      "Epoch 9/10 - Loss: 0.041914\n",
      "Epoch 10/10 - Loss: 0.040895\n",
      "INPUT 0: heir average albedo \n",
      "GOLD 0: array([0, 2, 0, 1, 2, 2, 0, 2, 1, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 2])\n",
      "PRED 0: array([0, 2, 0, 1, 2, 2, 0, 2, 1, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 2])\n",
      "INPUT 1: ed by rank and file \n",
      "GOLD 1: array([1, 1, 2, 0, 0, 2, 0, 1, 1, 0, 2, 1, 1, 1, 2, 0, 0, 0, 1, 2])\n",
      "PRED 1: array([1, 1, 2, 0, 0, 2, 0, 1, 1, 0, 2, 1, 1, 1, 2, 0, 0, 0, 1, 2])\n",
      "INPUT 2: s can also extend in\n",
      "GOLD 2: array([1, 2, 0, 1, 2, 2, 1, 0, 1, 0, 2, 1, 0, 0, 1, 2, 0, 2, 0, 2])\n",
      "PRED 2: array([1, 2, 0, 1, 2, 2, 1, 0, 1, 0, 2, 1, 0, 0, 1, 2, 0, 2, 0, 2])\n",
      "INPUT 3: erages between nine \n",
      "GOLD 3: array([2, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2])\n",
      "PRED 3: array([2, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2])\n",
      "INPUT 4:  that civilization n\n",
      "GOLD 4: array([2, 2, 0, 1, 2, 2, 0, 2, 0, 2, 0, 2, 0, 1, 2, 2, 0, 1, 2, 1])\n",
      "PRED 4: array([2, 2, 0, 1, 2, 2, 0, 2, 0, 2, 0, 2, 0, 1, 2, 2, 0, 1, 2, 1])\n",
      "Accuracy: 100 / 100 = 1.000000\n",
      "Training accuracy (100 exs):\n",
      "Accuracy: 1961 / 2000 = 0.980500\n",
      "Dev accuracy (whole set):\n",
      "Decoding on a large number of examples (1000); not printing or plotting\n",
      "Accuracy: 19775 / 20000 = 0.988750\n"
     ]
    }
   ],
   "source": [
    "!python letter_counting.py --task BEFOREAFTER "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medgemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
